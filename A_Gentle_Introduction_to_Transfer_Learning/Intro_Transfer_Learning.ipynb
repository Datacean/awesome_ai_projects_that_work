{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Gentle Introduction to Transfer Learning for Image Classification\n",
    "\n",
    "According to [Andrew Ng](http://www.andrewng.org/), cofounder of Coursera and Adjunct Professor at Stanford, Transfer Learning will be the next driver of Machine Learning commercial success.\n",
    "\n",
    "[Transfer Learning](http://cs231n.github.io/transfer-learning/) is a Machine Learning technique that allows to reutilize an already trained convolutional neural network (CNN) on a specific dataset and adapt it, or transfer it, to a different dataset. The reason you want to reuse a trained CNN is because it typically takes a long time to train. For example, training [ResNet18](https://arxiv.org/abs/1512.03385) for 30 epochs in 4 NVIDIA K80 GPU [took us 3 days](https://blogs.technet.microsoft.com/machinelearning/2016/11/15/imagenet-deep-neural-network-training-using-microsoft-r-server-and-azure-gpu-vms/). Training ResNet152 for 120 epochs in 4 NVIDIA K80 GPUs takes 4 months. \n",
    "\n",
    "In this post we will use [PyTorch](https://github.com/pytorch/pytorch/) to perform transfer learning in different datasets. We will explain the most common strategies to perform transfer learning and we will analyze when you should use each of them. \n",
    "\n",
    "**If you want to go directly to the results, scroll to the bottom of the notebook.**\n",
    "\n",
    "## Transfer learning strategies\n",
    "\n",
    "In general, there are two strategies to perform transfer learning, and I have not seen a final agreement on the naming. **Finetuning**, which consists of using the pretrained network on the base dataset and train all layers in the target dataset, and **freeze and train**, which consists of leaving all but the last layer frozen (the weights are not updated) and train the last layer. It is also possible to freeze the first couple of layers and finetune the rest, this is due to some evidence indicating that the first layers of the CNN contains [texture filters and color blobs](https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf). However, in this work we are going to analyze the two extreme cases: training all layers and training only the last layer.\n",
    "\n",
    "The most common base dataset is [ImageNet](http://image-net.org/), which contains 1.2 million images with 1000 categories. These categories are divided in two big groups, animals and objects. The number of images per category is around 1000. Most deep learning libraries provide CNN models pretrained on ImageNet.  \n",
    "\n",
    "The image below shows the workflow of a CNN architecture, i.e. ResNet, identifying a cat. The input is an image of 224x224px and 3 channels (RGB), and the output is the label `tabby cat`. The last layer has 1000 units, corresponding to the 1000 classes of ImageNet.\n",
    "![ResNet CNN](./img/resnet.svg)\n",
    "\n",
    "In the image below we can see the two mentioned strategies for transfer learning. Here we used a pretrained CNN on ImageNet and adapt it to classify Homer Simpson, using as the target dataset a subset of [the Simpsons Character Data](https://www.kaggle.com/alexattia/the-simpsons-characters-dataset). This subset contains 20 classes with between 300 and 1000 images per class. \n",
    "\n",
    "The first step to perform transfer learning is to adapt the number of classes from the base dataset to the target dataset. In the case of the Simpsons dataset, we have to remove the last layer of the network, which contains 1000 units and add a new layer with 20 units. \n",
    "\n",
    "Then we can use **freeze and train**, as it is represented in the upper figure, and just train the last layer, or we can **finetune** all layers, as it is represented in the bottom figure.\n",
    "![Transfer Learning](./img/finetuning_freezing.svg)\n",
    "\n",
    "## When to use transfer learning\n",
    "\n",
    "It is difficult to know in which cases one should just train the last layer or finetune the network. In [(Yosinsky et. al., 2014)](http://arxiv.org/abs/1411.1792), the authors address the problem of quantifying the degree to which a particular CNN layer is general or specific in the context of the ImageNet dataset. They found that the the transferability is negatively affected by splitting the network in the middle layers due to coadaptation of these layers. They reported that the transferability gap grows as the distance between tasks increases and finally, they found that initializing the network with transferred weights can improve generalization performance in comparison with training it from zero weights.\n",
    "\n",
    "As reported in this [tutorial of Karpathy](http://cs231n.github.io/transfer-learning/), these are some guidelines of the different scenarios when using transfer learning in a new dataset:\n",
    "* **Small and similar images**: When the target dataset is small in comparison with the base dataset and its images are very different, the recommendation is to freeze and train the last layer.\n",
    "* **Large and similar images**: When the dataset is large and it has similar images the recommendation is finetune. \n",
    "* **Small and different images**: In this case the recommendation is freeze and train the last layer or some of the last layers\n",
    "* **Large and different images**: In this case the recommendation is finetune.\n",
    "\n",
    "In this notebook, we are going to revisit these strategies using different datasets.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.backends.cudnn.benchmark=True # enables cudnn's auto-tuner\n",
    "\n",
    "from utils import (\n",
    "    get_gpu_name, \n",
    "    get_number_processors, \n",
    "    plot_pytorch_data_stream, \n",
    "    create_dataset, \n",
    "    available_models, \n",
    "    plot_metrics,\n",
    "    finetune,\n",
    "    freeze_and_train,\n",
    ")\n",
    "\n",
    "print(f\"OS: {sys.platform}\")\n",
    "print(f\"Python: {sys.version}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Numpy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"Number of CPU processors: {get_number_processors()}\")\n",
    "print(f\"GPU: {get_gpu_name()}\")\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and utilities\n",
    "\n",
    "The first step is to define the `finetune` and `freeze_and_train` routines. In addition, we list all the models avaialable in pytorch that we could potentially use as a base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = available_models()\n",
    "print(model_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `finetune` function loads a pretrained model, replaces the last layer with a new one matching the number of classes, and trains **all layers**:\n",
    "\n",
    "```python\n",
    "def finetune(dataloaders, model_name, sets, num_epochs, num_gpus, lr, momentum, lr_step, lr_epochs, verbose=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_class = len(dataloaders[sets[0]].dataset.class_to_idx)\n",
    "    model_ft = models.__dict__[model_name](weights=\"DEFAULT\")\n",
    "    num_ftrs = model_ft.fc.in_features\n",
    "    model_ft.fc = nn.Linear(num_ftrs, num_class)\n",
    "\n",
    "    if num_gpus > 1 and torch.cuda.device_count() > 1:\n",
    "        model_ft = nn.DataParallel(model_ft)\n",
    "    model_ft = model_ft.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model_ft.parameters(), lr=lr, momentum=momentum)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_epochs, gamma=lr_step)\n",
    "    model_ft = train_model(dataloaders, model_ft, sets, criterion, optimizer, exp_lr_scheduler,\n",
    "                           num_epochs=num_epochs, verbose=verbose)\n",
    "    return model_ft\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The `freeze_and_train` function freezes all layers except the last one, then trains **only the final classification layer**:\n",
    "\n",
    "```python\n",
    "def freeze_and_train(dataloaders, model_name, sets, num_epochs, num_gpus, lr, momentum, lr_step, lr_epochs, verbose=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    num_class = len(dataloaders[sets[0]].dataset.class_to_idx)\n",
    "    model_conv = models.__dict__[model_name](weights=\"DEFAULT\")\n",
    "    for param in model_conv.parameters():\n",
    "        param.requires_grad = False\n",
    "    num_ftrs = model_conv.fc.in_features\n",
    "    model_conv.fc = nn.Linear(num_ftrs, num_class)\n",
    "\n",
    "    if num_gpus > 1 and torch.cuda.device_count() > 1:\n",
    "        model_conv = nn.DataParallel(model_conv)\n",
    "    model_conv = model_conv.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    if num_gpus > 1 and torch.cuda.device_count() > 1:\n",
    "        params = model_conv.module.fc.parameters()\n",
    "    else:\n",
    "        params = model_conv.fc.parameters()\n",
    "    optimizer = SGD(params, lr=lr, momentum=momentum)\n",
    "    exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=lr_epochs, gamma=lr_step)\n",
    "    model_conv = train_model(dataloaders, model_conv, sets, criterion, optimizer, exp_lr_scheduler,\n",
    "                             num_epochs=num_epochs, verbose=verbose)\n",
    "    return model_conv\n",
    "```\n",
    "\n",
    "Both functions are imported from `utils.py` and used directly below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "We are going to use different datasets to test the transfer learning routines. We converted all of them to grayscale to analyze how the network behaves with in this color space. We also had to divide the dataset folders into `train` and `validation`, following the structure of pytorch. All the steps to do this are in the notebook [data_prep.ipynb](data_prep.ipynb).\n",
    "\n",
    "**Note:** The Simpsons and Dogs vs Cats datasets require a Kaggle API key. See the [README](README.md) for setup instructions.\n",
    "\n",
    "Links to source:\n",
    "* [Hymenoptera](https://download.pytorch.org/tutorial/hymenoptera_data.zip), 2 classes and 397 images.\n",
    "* [Simpons](https://www.kaggle.com/alexattia/the-simpsons-characters-dataset/data), 20 classes (subset of total) and 19548 images.\n",
    "* [Dogs vs Cats](https://www.kaggle.com/datasets/karakaggle/kaggle-cat-vs-dog-dataset), 2 classes and 25000 images.\n",
    "* [Caltech 256](https://data.caltech.edu/records/nyy15-4j048), 257 classes (yeah, I was also surprised) and 30607 images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DATA_ROOT = \"./data\"\n",
    "\n",
    "HYMENOPTERA_ROOT = os.path.join(DATA_ROOT, \"hymenoptera_data\")\n",
    "HYMENOPTERA_GRAY_ROOT = os.path.join(DATA_ROOT, \"hymenoptera_gray\")\n",
    "\n",
    "SIMPSONS_ROOT = os.path.join(DATA_ROOT, \"simpsons\", \"simpsons\")\n",
    "SIMPSONS_GRAY_ROOT = os.path.join(DATA_ROOT, \"simpsons\", \"simpsons_gray\")\n",
    "\n",
    "DOGS_CATS_ROOT = os.path.join(DATA_ROOT, \"dogs_vs_cats\", \"dogs_vs_cats\")\n",
    "DOGS_CATS_GRAY_ROOT = os.path.join(DATA_ROOT, \"dogs_vs_cats\", \"dogs_vs_cats_gray\")\n",
    "\n",
    "CALTECH256_ROOT = os.path.join(DATA_ROOT, \"caltech256\", \"caltech256\")\n",
    "CALTECH256_GRAY_ROOT = os.path.join(DATA_ROOT, \"caltech256\", \"caltech256_gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the initial parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MODEL_NAME = \"resnet18\" # \"resnet152\"\n",
    "BATCH_SIZE = 64\n",
    "SETS = [\"train\", \"val\"]\n",
    "NUM_GPUS = 1\n",
    "EPOCHS = 15\n",
    "LR = 0.001\n",
    "LR_STEP = 0.1\n",
    "LR_EPOCHS = 10\n",
    "MOMENTUM = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to load each of the datasets, showing some internal stats for each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_hymenoptera = create_dataset(HYMENOPTERA_ROOT, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pytorch_data_stream(data_hymenoptera[\"train\"], max_images=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_hymenoptera_gray = create_dataset(HYMENOPTERA_GRAY_ROOT, batch_size=BATCH_SIZE, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pytorch_data_stream(data_hymenoptera_gray[\"train\"], max_images=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_simpsons = create_dataset(SIMPSONS_ROOT, batch_size=BATCH_SIZE*NUM_GPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pytorch_data_stream(data_simpsons[\"train\"], max_images=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_simpsons_gray = create_dataset(SIMPSONS_GRAY_ROOT, batch_size=BATCH_SIZE*NUM_GPUS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pytorch_data_stream(data_simpsons_gray[\"train\"], max_images=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dogs_vs_cats = create_dataset(DOGS_CATS_ROOT, batch_size=BATCH_SIZE*NUM_GPUS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pytorch_data_stream(data_dogs_vs_cats[\"train\"], max_images=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dogs_vs_cats_gray = create_dataset(DOGS_CATS_GRAY_ROOT, batch_size=BATCH_SIZE*NUM_GPUS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pytorch_data_stream(data_dogs_vs_cats_gray[\"train\"], max_images=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_caltech256 = create_dataset(CALTECH256_ROOT, batch_size=BATCH_SIZE*NUM_GPUS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pytorch_data_stream(data_caltech256[\"train\"], max_images=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_caltech256_gray = create_dataset(CALTECH256_GRAY_ROOT, batch_size=BATCH_SIZE*NUM_GPUS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pytorch_data_stream(data_caltech256_gray[\"train\"], max_images=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "For each dataset, we are going to compute finetunining and freeze and train. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_acc_ft = \"Validation accuracy finetuning\"\n",
    "val_acc_fr = \"Validation accuracy freezing\"\n",
    "df = pd.DataFrame(columns=[val_acc_ft, val_acc_fr])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hymenoptera dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = finetune(data_hymenoptera, MODEL_NAME, SETS, EPOCHS, 1, LR, \n",
    "                          MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Dataset Hymenoptera\"\n",
    "plot_metrics(metrics, dataset_name + \" (finetuning)\")\n",
    "df.at[dataset_name, val_acc_ft] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = freeze_and_train(data_hymenoptera, MODEL_NAME, SETS, EPOCHS, 1, LR, \n",
    "                                  MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics, dataset_name + \" (freeze and train)\")\n",
    "df.at[dataset_name, val_acc_fr] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = finetune(data_hymenoptera_gray, MODEL_NAME, SETS, EPOCHS, 1, LR, \n",
    "                          MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Dataset Hymenoptera gray\"\n",
    "plot_metrics(metrics, dataset_name + \" (finetuning)\")\n",
    "df.at[dataset_name, val_acc_ft] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = freeze_and_train(data_hymenoptera_gray, MODEL_NAME, SETS, EPOCHS, 1, LR, \n",
    "                                  MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics, dataset_name + \" (freeze and train)\")\n",
    "df.at[dataset_name, val_acc_fr] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Simpsons dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = finetune(data_simpsons, MODEL_NAME, SETS, EPOCHS, NUM_GPUS, LR, \n",
    "                          MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Dataset Simpsons\"\n",
    "plot_metrics(metrics, dataset_name + \" (finetuning)\")\n",
    "df.at[dataset_name, val_acc_ft] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = freeze_and_train(data_simpsons, MODEL_NAME, SETS, EPOCHS, NUM_GPUS, LR, \n",
    "                                  MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics, dataset_name + \" (freeze and train)\")\n",
    "df.at[dataset_name, val_acc_fr] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = finetune(data_simpsons_gray, MODEL_NAME, SETS, EPOCHS, NUM_GPUS, LR, \n",
    "                          MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Dataset Simpsons gray\"\n",
    "plot_metrics(metrics, dataset_name + \" (finetuning)\")\n",
    "df.at[dataset_name, val_acc_ft] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = freeze_and_train(data_simpsons_gray, MODEL_NAME, SETS, EPOCHS, NUM_GPUS, LR, \n",
    "                                  MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics, dataset_name + \" (freeze and train)\")\n",
    "df.at[dataset_name, val_acc_fr] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Dogs vs cats dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = finetune(data_dogs_vs_cats, MODEL_NAME, SETS, EPOCHS, NUM_GPUS, LR, \n",
    "                          MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Dataset Dogs vs Cats\"\n",
    "plot_metrics(metrics, dataset_name + \" (finetuning)\")\n",
    "df.at[dataset_name, val_acc_ft] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = freeze_and_train(data_dogs_vs_cats, MODEL_NAME, SETS, EPOCHS, NUM_GPUS, LR, \n",
    "                                  MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics, dataset_name + \" (freeze and train)\")\n",
    "df.at[dataset_name, val_acc_fr] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = finetune(data_dogs_vs_cats_gray, MODEL_NAME, SETS, EPOCHS, NUM_GPUS, LR, \n",
    "                          MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Dataset Dogs vs Cats gray\"\n",
    "plot_metrics(metrics, dataset_name + \" (finetuning)\")\n",
    "df.at[dataset_name, val_acc_ft] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = freeze_and_train(data_dogs_vs_cats_gray, MODEL_NAME, SETS, EPOCHS, NUM_GPUS, LR, \n",
    "                                  MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics, dataset_name + \" (freeze and train)\")\n",
    "df.at[dataset_name, val_acc_fr] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caltech256 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = finetune(data_caltech256, MODEL_NAME, SETS, EPOCHS, NUM_GPUS, LR, \n",
    "                          MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Dataset Caltech256\"\n",
    "plot_metrics(metrics, dataset_name + \" (finetuning)\")\n",
    "df.at[dataset_name, val_acc_ft] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = freeze_and_train(data_caltech256, MODEL_NAME, SETS, EPOCHS, NUM_GPUS, LR, \n",
    "                                  MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics, dataset_name + \" (freeze and train)\")\n",
    "df.at[dataset_name, val_acc_fr] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = finetune(data_caltech256_gray, MODEL_NAME, SETS, EPOCHS, NUM_GPUS, LR, \n",
    "                          MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Dataset Caltech256 gray\"\n",
    "plot_metrics(metrics, dataset_name + \" (finetuning)\")\n",
    "df.at[dataset_name, val_acc_ft] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, metrics = freeze_and_train(data_caltech256_gray, MODEL_NAME, SETS, EPOCHS, NUM_GPUS, LR, \n",
    "                                  MOMENTUM, LR_STEP, LR_EPOCHS, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(metrics, dataset_name + \" (freeze and train)\")\n",
    "df.at[dataset_name, val_acc_fr] = max(metrics[\"val_acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion and analysis\n",
    "\n",
    "In this notebook we used a limited number of datasets with a small network, ResNet18, so it will be premature to generalize the findings to all datasets and networks. However, the findings may shed some light on the problem of when to use transfer learning. In the following table there is a summary of the results.\n",
    "\n",
    "| Dataset | # of classes | # of images | Val. accuracy finetuning | Val. accuracy freezing |\n",
    "| -- | :--: | :--: | :--: | :--: | \n",
    "| Dataset Hymenoptera | 2 | 397 | 0.954248 | 0.960784 |\n",
    "| Dataset Hymenoptera gray | 2 | 397 | 0.921569 | 0.901961 |\n",
    "| Dataset Simpsons | 20 | 19548 | 0.924552 | 0.641944 |\n",
    "| Dataset Simpsons gray | 20 | 19548 | 0.901535 | 0.543223 |\n",
    "| Dataset Dogs vs Cats | 2 | 25000 | 0.9894 | 0.981 |\n",
    "| Dataset Dogs vs Cats gray | 2 | 25000 | 0.9888 | 0.9756 |\n",
    "| Dataset Caltech256 | 257 | 30607 | 0.673643 | 0.542511 |\n",
    "| Dataset Caltech256 gray | 257 | 30607 | 0.612819 | 0.434925 |\n",
    "\n",
    "The first detail we observe is that degradation of accuracy when training a gray scale dataset in comparison with its color scale counterpart. This is consistent with the observation of other authors that the more different the domain between the base dataset and the target dataset, the worse the transference. \n",
    "\n",
    "We see as well that for Simpsons and Caltech256 datasets there is a high degradation when freezing. The highest degradation can be appreciated in Simpson dataset, which can be due to the fact that the domains are quite different, while in ImageNet there are natural images, in Simpsons the images have mostly plain colors. In Caltech we observe firstly a low accuracy, apart from a high degradation when freezing. Maybe the reason is because for a dataset of a high number of classes, the number of images per class is small, in the order of a couple of houndreds. \n",
    "\n",
    "The domain of dogs vs cats dataset is the closest to ImageNet, in fact, ImageNet contains several breeds of dogs and cats. In this case, there is no much difference when finetuning or freezing.\n",
    "\n",
    "Finally, in the hymenoptera dataset we see a small improvement in the color dataset when freezing. This can be because the domain is closer and the dataset is small. In the grayscale counterpart we don't see an improvement when freezing, probably due to the domain difference. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "datacean (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
