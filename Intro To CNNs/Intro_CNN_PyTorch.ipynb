{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Convolutional Neural Networks\n",
    "\n",
    "Convolutional Neural Networks (CNN) are one of the key components in the success of Deep Learning and the new Artificial Intelligence revolution. They are specially advantageous in tasks such as object detection, scene understanding and, recently, natural language processing. In this jupyter notebook I will explain what is a convolution and how to train a CNN with the character recognition dataset MNIST.\n",
    "\n",
    "*NOTE: Make sure your PyTorch version is compatible with your CUDA drivers. In my case, I installed `pip install torch torchvision --index-url https://download.pytorch.org/whl/cu116`*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      4\u001b[39m logger.setLevel(logging.INFO)\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msys\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "# Load all needed libraries\n",
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "import sys\n",
    "import cv2\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "print(\"OS:\", sys.platform)\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Numpy:\", np.__version__)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"CUDA:\", torch.cuda.is_available())\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution\n",
    "\n",
    "CNN are the key resource in deep learning. They are based on a mathematical operation called convolution. A convolution is just a multiplication of an input image (which is a matrix) times a kernel (which is another matrix).\n",
    "\n",
    "In opencv there is a function called filter2D that allows to generate convolutions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_image(image, image2=None):\n",
    "    # Show one image\n",
    "    plt.subplot(121)\n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(image)\n",
    "    else:\n",
    "        plt.imshow(image, cmap=plt.get_cmap(\"gray\"))\n",
    "    plt.axis(\"off\")\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if image2 is not None:\n",
    "        # Show two images\n",
    "        plt.subplot(122)\n",
    "        if len(image2.shape) == 3:\n",
    "            image2 = cv2.cvtColor(image2, cv2.COLOR_BGR2RGB)\n",
    "            plt.imshow(image2)\n",
    "        else:\n",
    "            plt.imshow(image2, cmap=plt.get_cmap(\"gray\"))\n",
    "        plt.axis(\"off\")\n",
    "        plt.xticks([]), plt.yticks([])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "im = cv2.imread(\"Lenna.png\")\n",
    "plot_image(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Sharpening filter\n",
    "sharpen = np.array(([0, -1, 0], [-1, 5, -1], [0, -1, 0]), dtype=\"int\")\n",
    "# Laplacian kernel used to detect edges\n",
    "laplacian = np.array(([0, 1, 0], [1, -4, 1], [0, 1, 0]), dtype=\"int\")\n",
    "# Sobel x-axis kernel\n",
    "sobelX = np.array(([-1, 0, 1], [-2, 0, 2], [-1, 0, 1]), dtype=\"int\")\n",
    "# Sobel y-axis kernel\n",
    "sobelY = np.array(([-1, -2, -1], [0, 0, 0], [1, 2, 1]), dtype=\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dst = cv2.filter2D(im, -1, sharpen)\n",
    "plot_image(im, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dst = cv2.filter2D(im, -1, laplacian)\n",
    "plot_image(im, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dst = cv2.filter2D(im, -1, sobelX)\n",
    "plot_image(im, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dst = cv2.filter2D(im, -1, sobelY)\n",
    "plot_image(im, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character recognition with MNIST dataset\n",
    "\n",
    "We are going to use MNIST dataset and Lenet CNN architecture to showcase a deep learning task consisting in recognizing handwritting characters. \n",
    "\n",
    "[MNIST](http://yann.lecun.com/exdb/mnist/) dataset contains a training set of 60,000 examples, and a test set of 10,000 examples of hand writting characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "mnist = fetch_openml(\"mnist_784\", version=1)\n",
    "\n",
    "# Convert the data and target to NumPy arrays\n",
    "X = mnist.data.to_numpy()\n",
    "Y = mnist.target.to_numpy()\n",
    "\n",
    "# Shuffle the data\n",
    "np.random.seed(1234)\n",
    "p = np.random.permutation(X.shape[0])  # Get permutation of indices\n",
    "X = X[p].astype(np.float32) / 255  # Normalize the data\n",
    "Y = Y[p].astype(int)\n",
    "\n",
    "# Visualize the first 10 images\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.imshow(X[i].reshape((28, 28)), cmap=\"Greys_r\")\n",
    "    plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Train/test split and reshape to PyTorch format (batch_size, channels, height, width)\n",
    "X_train = X[:60000].reshape((-1, 1, 28, 28))\n",
    "X_test = X[60000:].reshape((-1, 1, 28, 28))\n",
    "Y_train = Y[:60000]\n",
    "Y_test = Y[60000:]\n",
    "\n",
    "# Create PyTorch datasets and loaders\n",
    "batch_size = 100\n",
    "train_dataset = TensorDataset(torch.tensor(X_train), torch.tensor(Y_train))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test), torch.tensor(Y_test))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN model: Lenet\n",
    "\n",
    "Lenet architecture was published by [Yann LeCun et al.](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) in 1998. The LeNet architecture uses convolutions and pooling to increase the performance. For many years, LeNet was the most accurate algorithm for character recognition and supposed a great advance in deep neural networks, long before the appearance of GPUs and CUDA.\n",
    "\n",
    "As it can be seen in the following code Lenet has 4 groups of hidden layers, two convolutions and two fully connected layers. Each convolution is followed by an activation and a pooling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # Conv1: 1 input channel (grayscale), 20 output feature maps, 5x5 kernel\n",
    "        self.conv1 = nn.Conv2d(1, 20, kernel_size=5)\n",
    "        # Conv2: 20 input feature maps, 50 output feature maps, 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(20, 50, kernel_size=5)\n",
    "        # FC1: 50 filters with 4x4 feature map after pooling\n",
    "        self.fc1 = nn.Linear(50 * 4 * 4, 500)\n",
    "        # FC2: 500 input features, 10 output features (for 10 classes)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = torch.tanh(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = x.view(-1, 50 * 4 * 4)  # Flatten the tensor\n",
    "        x = torch.tanh(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)  # Softmax for output layer\n",
    "\n",
    "\n",
    "# Instantiate the model\n",
    "model = LeNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Once we have the model and the data, let's put it together and train the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss function\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# Training loop\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(\n",
    "                f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} \"\n",
    "                f\"({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\"\n",
    "            )\n",
    "\n",
    "\n",
    "# Testing loop\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()  # Sum up batch loss\n",
    "            pred = output.argmax(\n",
    "                dim=1, keepdim=True\n",
    "            )  # Get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(\n",
    "        f\"\\nTest set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(test_loader.dataset)} \"\n",
    "        f\"({100. * correct / len(test_loader.dataset):.0f}%)\\n\"\n",
    "    )\n",
    "\n",
    "\n",
    "# Train for 10 epochs\n",
    "for epoch in range(1, 11):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
