{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3424620c",
   "metadata": {},
   "source": [
    "# Gradient Boosting Decision Trees\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f02f20ad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m make_blobs\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DecisionTreeClassifier\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlgb\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m; sns.set()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings (\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from utils import visualize_classifier, visualize_tree_sklearn, visualize_tree_lightgbm\n",
    "\n",
    "print(f\"Numpy version:{np.__version__}\")\n",
    "print(f\"Pandas version:{pd.__version__}\")\n",
    "print(f\"Sklearn version:{sklearn.__version__}\")\n",
    "print(f\"LightGBM version:{lgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04f6726",
   "metadata": {},
   "source": [
    "## Decision Trees\n",
    "\n",
    "A Decision Tree is a machine learning algorithm that can be used for classification or regression. The intiution behind Decision Trees is that you split the data into regions until all data points belong to each class is inside their own region.\n",
    "\n",
    "Some details of Decision Trees:\n",
    "* DT models are fairly intuitive and easy to explain to business.\n",
    "* DT cut feature space in rectangles [or (hyper)cuboids].\n",
    "* DT any number of categorical variables are not really an issue.\n",
    "* DT overfit easily. To solve that, you can use Random Forest or Boosted Decision Trees.\n",
    "* No need to normalize the features.\n",
    "\n",
    "To train a Decision Tree we optimize a loss function that finds the optimal split. The optimal split is the one that minimizes the classification error. This is measured in terms of \"purity\", a pure node happens when all the elements in the node are of one unique class. There are two ways of measuring the purity of a node, using the **Gini index** or using **entropy**.\n",
    "\n",
    "#### Gini index:\n",
    "\n",
    "The Gini index is defined:\n",
    "\n",
    "$$\n",
    "Gini (node) = \\sum_{k=1}^c p_k(1-p_k) \n",
    "$$\n",
    "\n",
    "where $p_k$ is the probability of picking a data poing from class k.\n",
    "\n",
    "Details:\n",
    "* Gini is a measure of variance in the node.\n",
    "* Gini is high if there are many data points belonging to the wrong class.\n",
    "* Gini is zero if all the data points belong to the same class. \n",
    "* Gini is faster to compute than Entropy.\n",
    "* Also used in economics to measure inequality.\n",
    "\n",
    "#### Entropy\n",
    "\n",
    "The entropy is defined as:\n",
    "\n",
    "$$\n",
    "Entropy (node) = \\sum_{k=1}^c p_k log_2 (p_k)\n",
    "$$\n",
    "\n",
    "where $p_k$ is the probability of picking a data point from class k.\n",
    "\n",
    "Details:\n",
    "* Entropy is a measure of uncertainty in the node.\n",
    "* Entropy is high if there are many data points belonging to the wrong class.\n",
    "* Entropy is zero if all the data points belong to the same class. \n",
    "* Entropy is slower to compute than Gini.\n",
    "* It is used in statistics to measure the expected amount of information that can be drawn from a distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827edbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc08a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion=\"gini\", random_state=42, max_depth=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a6c2f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(X, y)\n",
    "print(f\"Tree depth: {model.get_depth()}\")\n",
    "print(f\"Number of leaves: {model.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0162cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classifier(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4ed945",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree_sklearn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe9bacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion=\"gini\", random_state=42, max_depth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33042c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(X, y)\n",
    "print(f\"Tree depth: {model.get_depth()}\")\n",
    "print(f\"Number of leaves: {model.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9115c337",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classifier(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bcf83b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree_sklearn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80539cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion=\"gini\", random_state=42, max_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d410b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(X, y)\n",
    "print(f\"Tree depth: {model.get_depth()}\")\n",
    "print(f\"Number of leaves: {model.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a516a179",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classifier(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92ce580",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree_sklearn(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6231caf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4, random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49f75a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion=\"gini\", random_state=42, max_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec43e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(X, y)\n",
    "print(f\"Tree depth: {model.get_depth()}\")\n",
    "print(f\"Number of leaves: {model.get_n_leaves()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9e2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classifier(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89482dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree_sklearn(model, figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d375c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier(criterion=\"gini\", random_state=42, max_depth=3)\n",
    "model = model.fit(X, y)\n",
    "print(f\"Tree depth: {model.get_depth()}\")\n",
    "print(f\"Number of leaves: {model.get_n_leaves()}\")\n",
    "visualize_classifier(model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fde220b",
   "metadata": {},
   "source": [
    "## Gradient Boosting Decision Trees\n",
    "\n",
    "Gradient boosting is a machine learning technique that produces a prediction model in the form of an ensemble of weak classifiers. One of the most popular types of gradient boosting is gradient boosted decision trees (GBDT), that internally is made up of an ensemble of weak decision trees.\n",
    "\n",
    "The two most popular GBDT frameworks are XGBoost and LightGBM. Both frameworks uses efficient and scalable implementations for gradient boosting that can be used for classification and regression.\n",
    "\n",
    "Boosting is a technique that builds strong classifiers by sequentially emsambling weak ones. First, a model is trained on the data. Then a second model tries to correct the errors found in the first model. This process is repeated until the error is reduced to a limit or a maximum number of models is added.\n",
    "\n",
    "In ADA boost, we take the data points that had bad performance and they are weighted more in the next model. In gradient boosting, we take the residual, which is the difference between the true labels $y$ and the labels predicted by the model $\\hat y$.\n",
    "\n",
    "* For the first estimator, the residual is $y - f_1(x)$.\n",
    "* For the second estimator, the residual is $y - f_1(x) - f_2(x)$.\n",
    "* For the $n^{th}$ estimator, the residual is $y - \\sum_{i=1}^n f_n(x)$\n",
    "\n",
    "To train the GBDT, we want to minimize a loss which the difference between the true and predicted labels and we add a regularization term $\\Omega$ to reduce the complexity of the model.\n",
    "\n",
    "$$ \\mathcal {L} = \\sum_{i=1}^n l(y_i, \\hat y_i) + \\sum_{i=1}^n \\Omega (f_i)$$\n",
    "\n",
    "\n",
    "The loss term $\\sum_{i=1}^n l(y_i, \\hat y_i)$ is not easy to compute, one way is via a Taylor expansion. A Taylor expansion is a way to approximate a complex function based on its derivative (gradient or Jacobian) and second derivate (Hessian). In particular, the hessian is not easy to compute, and this is one of the reasons finding the right split in gradient boosting trees can be difficult.\n",
    "\n",
    "To find the optimal split, instead of computing the Gini or the entropy, GBDT computes a gain that has into account the gradient, the hessian and other terms.\n",
    "\n",
    "There are two different strategies to compute the trees: level-wise and leaf-wise. \n",
    "\n",
    "<img src=\"./img/DecisionTrees_3_thumb.png\" />\n",
    "\n",
    "The level-wise strategy grows the tree level by level. In this strategy, each node splits the data prioritizing the nodes closer to the tree root. The leaf-wise strategy grows the tree by splitting the data at the nodes with the highest loss change. \n",
    "\n",
    "Level-wise growth is usually better for smaller datasets whereas leaf-wise tends to overfit. Leaf-wise growth tends to excel in larger datasets where it is considerably faster than level-wise growth.\n",
    "\n",
    "A key challenge in training boosted decision trees is the computational cost of finding the best split for each leaf. Conventional techniques find the exact split for each leaf, and require scanning through all the data in each iteration. \n",
    "\n",
    "A different approach approximates the split by building histograms of the features. That way, the algorithm doesn't need to evaluate every single value of the features to compute the split, but only the bins of the histogram, which are bounded. This approach turns out to be much more efficient for large datasets, without adversely affecting accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c50bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f01c3325",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(boosting_type=\"gbdt\", random_state=42, max_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b037b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b699e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classifier(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907a5373",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree_lightgbm(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bacd4e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4, random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7f9d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(boosting_type=\"gbdt\", random_state=42, max_depth=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5848e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f34f61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classifier(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6328c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree_lightgbm(model, figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897c6dcd",
   "metadata": {},
   "source": [
    "## Random Forests\n",
    "\n",
    "Random Forest is a machine learning model that can be used for general classification or regression problems. It is an improvement on top of Decision Trees and help to overcome the overfitting of these models.\n",
    "\n",
    "In particular, Random Forest use a technique called bagging, short of boostrap aggregation. The data is randomly divided in a number of bags, and on each bag we put around 60% of the data. Then we train a decision tree on each bag and compute the ensemble of these models. The training of Random Forest is done in parallel.\n",
    "\n",
    "An interesting detail is that when we build the subsample the data to generate the bags, we add the data with replacement (this means that the same data can go to the same bag). Adding data with replacement ensures that we don't run out of datapoints, and that each bag is large enough to be a good representation of the full dataset.\n",
    "\n",
    "Bagging is actually independent of the algorithm, even though it is used in Random Forest, it can also be used in Gradient Boosting Decision Trees or other machine learning algorithms.\n",
    "\n",
    "In terms of training, a Decision Tree is trained on each of the bags. For finding the optimal split, we use the same measures of purity that are used in individual Decision Trees, the Gini index or the entropy. \n",
    "\n",
    "Gini index is a meassure of the variance of datapoints on each node. If we have a lot of data point belonging to the wrong class, the Gini index is high, if all the data points belong to the same class, Gini is zero. \n",
    "\n",
    "The other way of computing the optimal split is using entropy. Entropy takes a probabilistic approach and treat each node as a probability distribution of the data. It measures the uncertainty of the node, if there are a lot of data points of the wrong class, the node doesn't have a lot of certainty and the entropy is high. However, if all the datapoints belong to the same class, the node has complete certainty about the class of the data, and the entropy is zero. \n",
    "\n",
    "After all the trees have been trained on each bag, we create an ensamble of all trees. This is a mayority vote, we ask a prediction for each tree and get the most voted class. Using ensambles is a very good way of improving generalization and reducing overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4d81da",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4, random_state=42, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7bd21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(boosting_type=\"random_forest\", random_state=42, max_depth=None, bagging_fraction=0.6, bagging_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0787da6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2352f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classifier(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15987a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree_lightgbm(model, figsize=(15,15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9848b962",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=300, centers=4, random_state=0, cluster_std=1.0)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='rainbow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17326a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = lgb.LGBMClassifier(boosting_type=\"random_forest\", random_state=42, max_depth=None, bagging_fraction=0.6, bagging_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d38ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "551f4ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_classifier(model, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a755e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tree_lightgbm(model, figsize=(15,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b9e3d",
   "metadata": {},
   "source": [
    "## Practical tricks to excel with LightGBM\n",
    "\n",
    "One of the biggest problems of LightGBM is that it has a large number of parameters that can be tuned. More than 130 according to their [documentation](https://lightgbm.readthedocs.io/en/latest/Parameters.html).\n",
    "\n",
    "Some advice can be found in their [parameter tuning site](https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html#deal-with-over-fitting).\n",
    "\n",
    "**For better accuracy**\n",
    "* Use large `max_bin`. LightGBM use histograms to approximate speed up the data split operation, the higher the number of bins, the more the histogram is similar to the real data, but the slower the computation.\n",
    "* Use small `learning_rate` with `large num_iterations`. This slows down the training but it make the model to learn better.\n",
    "* Use large `num_leaves`. May cause overfitting, the model is more complex, but the accuracy will be better.\n",
    "* Use a bigger training dataset\n",
    "* Try `dart` algorithm. [DART](https://arxiv.org/abs/1505.01866) is a method that uses dropout, very common in Neural Networks, to improve the model generalization.\n",
    "\n",
    "**For dealing with overfitting**\n",
    "* Use small `max_bin`. Less histogram bins makes the split process faster.\n",
    "* Use small `num_leaves`. Less leaves enforces the model to reduce its complexity and generalize better.\n",
    "* Use `min_data_in_leaf` and `min_sum_hessian_in_leaf`. Can help with generalization by making sure that each leaf has a minimum number of datapoints to avoid outliers.\n",
    "* Use bagging by setting `bagging_fraction` and `bagging_freq`. We subsample the dataset and train each tree with a fraction of the data, this protects against overfitting and increases the training speed.\n",
    "* Use feature sub-sampling by set `feature_fraction`. Instead of sub-sampling the data, we select a fraction of the features in each iteration. This can also improve generalization, but if the features are very different quantitatively between each other, this method can cause problems. \n",
    "* Use a bigger training dataset.\n",
    "* Try `lambda_l1`, `lambda_l2` and `min_gain_to_split` for regularization. Finding a good regularization value improve generalization.\n",
    "* Try `max_depth` to avoid growing deep tree. Complex trees tend to overfit. \n",
    "* Try `extra_trees`. Extremely randomize tree method select the split by randomly choosing only one threshold for each feature. This increases the training speed and reduces overfitting.\n",
    "* Try increasing `path_smooth`. This prevents the model to create leaves with few samples. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bfd568",
   "metadata": {},
   "source": [
    "## References\n",
    "* Decision Trees vs LightGBM experiments: https://mljar.com/machine-learning/decision-tree-vs-lightgbm/\n",
    "* Lessons Learned From Benchmarking Fast Machine Learning Algorithms: https://docs.microsoft.com/en-us/archive/blogs/machinelearning/lessons-learned-benchmarking-fast-machine-learning-algorithms\n",
    "* Decision Tree Learning https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "* Gradient Boosting https://en.wikipedia.org/wiki/Gradient_boosting\n",
    "* LightGBM parameter documentation: https://lightgbm.readthedocs.io/en/latest/Parameters.html\n",
    "* LightGBM parameter tuning documentation: https://lightgbm.readthedocs.io/en/latest/Parameters-Tuning.html#deal-with-over-fitting\n",
    "* Understanding LightGBM Parameters (and How to Tune Them) https://neptune.ai/blog/lightgbm-parameters-guide"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacean",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
